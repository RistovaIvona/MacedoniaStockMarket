{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import itertools\n",
    "import regex as re\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Activation\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tmp/mse_raw.csv', parse_dates=['date', 'start_date'], index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>average</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>quantity</th>\n",
       "      <th>change %</th>\n",
       "      <th>volume total</th>\n",
       "      <th>start_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALK</td>\n",
       "      <td>1997-01-09</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>279270.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279270.0</td>\n",
       "      <td>1997-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ALK</td>\n",
       "      <td>1997-01-10</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ALK</td>\n",
       "      <td>1997-01-11</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ALK</td>\n",
       "      <td>1997-01-12</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-01-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>ALK</td>\n",
       "      <td>1997-01-13</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997-01-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id       date    open    high     low  average   close    volume  \\\n",
       "0       ALK 1997-01-09  1070.0  1070.0  1070.0   1070.0  1070.0  279270.0   \n",
       "22      ALK 1997-01-10  1070.0  1070.0  1070.0   1070.0  1070.0       0.0   \n",
       "44      ALK 1997-01-11  1070.0  1070.0  1070.0   1070.0  1070.0       0.0   \n",
       "66      ALK 1997-01-12  1070.0  1070.0  1070.0   1070.0  1070.0       0.0   \n",
       "88      ALK 1997-01-13  1070.0  1070.0  1070.0   1070.0  1070.0       0.0   \n",
       "\n",
       "    quantity  change %  volume total start_date  \n",
       "0      261.0       0.0      279270.0 1997-01-09  \n",
       "22       0.0       0.0           0.0 1997-01-09  \n",
       "44       0.0       0.0           0.0 1997-01-09  \n",
       "66       0.0       0.0           0.0 1997-01-09  \n",
       "88       0.0       0.0           0.0 1997-01-09  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting RNN ALK dataset for training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alk_df = df[df.stock_id == 'ALK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['stock_id', 'date', 'open', 'high', 'low', 'average', 'close', 'volume',\n",
       "       'quantity', 'change %', 'volume total', 'start_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alk_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe with only closing price column\n",
    "close_price = alk_df['close']\n",
    "\n",
    "#Convert the dataframe into numpy array\n",
    "close_price = close_price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8630,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = scaler.fit_transform(close_price.reshape(len(close_price), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04175824],\n",
       "       [0.04175824],\n",
       "       [0.04175824],\n",
       "       ...,\n",
       "       [0.86373626],\n",
       "       [0.86373626],\n",
       "       [0.85714286]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### univariate sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04175824]\n",
      " [0.04175824]\n",
      " [0.04175824]] [0.04175824]\n",
      "[[0.04175824]\n",
      " [0.04175824]\n",
      " [0.04175824]] [0.04175824]\n",
      "[[0.04175824]\n",
      " [0.04175824]\n",
      " [0.04175824]] [0.04029304]\n"
     ]
    }
   ],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(scaled_df, n_steps)\n",
    "# summarize the data\n",
    "for i in range(3):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM, one day prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = X[:int(len(X)-60),], X[int(len(X)-60):int(len(X)-1),], X[int(len(X)-1):,]\n",
    "y_train, y_valid, y_test = y[:int(len(X)-60),], y[int(len(X)-60):int(len(X)-1),], y[int(len(X)-1):,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8567, 3, 1), (59, 3, 1), (1, 3, 1), (8567, 1), (59, 1), (1, 1))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape, y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_valid = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "rmse = RootMeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callback1 = ModelCheckpoint('vanila_best.pt', verbose=1, save_best_only=True, mode='min', monitor='val_loss')\n",
    "my_callback2 = EarlyStopping(patience=10)\n",
    "\n",
    "my_callbacks = [my_callback1, my_callback2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8567 samples, validate on 59 samples\n",
      "Epoch 1/50\n",
      "8567/8567 [==============================] - 2s 259us/step - loss: 0.0131 - val_loss: 1.6283e-04\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00016, saving model to vanila_best.pt\n",
      "Epoch 2/50\n",
      "8567/8567 [==============================] - 2s 185us/step - loss: 7.8980e-05 - val_loss: 6.0614e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00016 to 0.00006, saving model to vanila_best.pt\n",
      "Epoch 3/50\n",
      "8567/8567 [==============================] - 2s 203us/step - loss: 7.7059e-05 - val_loss: 6.0599e-05\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00006 to 0.00006, saving model to vanila_best.pt\n",
      "Epoch 4/50\n",
      "8567/8567 [==============================] - 2s 193us/step - loss: 7.7404e-05 - val_loss: 6.5257e-05\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00006\n",
      "Epoch 5/50\n",
      "8567/8567 [==============================] - 2s 188us/step - loss: 7.5061e-05 - val_loss: 7.9386e-05\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00006\n",
      "Epoch 6/50\n",
      "8567/8567 [==============================] - 2s 190us/step - loss: 7.5258e-05 - val_loss: 1.2660e-04\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00006\n",
      "Epoch 7/50\n",
      "8567/8567 [==============================] - 2s 198us/step - loss: 7.4946e-05 - val_loss: 8.2435e-05\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00006\n",
      "Epoch 8/50\n",
      "8567/8567 [==============================] - 2s 190us/step - loss: 7.6347e-05 - val_loss: 1.0144e-04\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00006\n",
      "Epoch 9/50\n",
      "8567/8567 [==============================] - 2s 195us/step - loss: 7.3816e-05 - val_loss: 6.4614e-05\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00006\n",
      "Epoch 10/50\n",
      "8567/8567 [==============================] - 2s 189us/step - loss: 7.4949e-05 - val_loss: 9.1933e-05\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00006\n",
      "Epoch 11/50\n",
      "8567/8567 [==============================] - 2s 188us/step - loss: 7.3992e-05 - val_loss: 5.6083e-05\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00006 to 0.00006, saving model to vanila_best.pt\n",
      "Epoch 12/50\n",
      "8567/8567 [==============================] - 2s 188us/step - loss: 7.2856e-05 - val_loss: 1.5146e-04\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00006\n",
      "Epoch 13/50\n",
      "8567/8567 [==============================] - 2s 193us/step - loss: 7.2116e-05 - val_loss: 7.4719e-05\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00006\n",
      "Epoch 14/50\n",
      "8567/8567 [==============================] - 2s 189us/step - loss: 7.2055e-05 - val_loss: 1.3754e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00006\n",
      "Epoch 15/50\n",
      "8567/8567 [==============================] - 2s 227us/step - loss: 7.2119e-05 - val_loss: 1.5465e-04\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00006\n",
      "Epoch 16/50\n",
      "8567/8567 [==============================] - 2s 204us/step - loss: 7.2157e-05 - val_loss: 6.1560e-05\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00006\n",
      "Epoch 17/50\n",
      "8567/8567 [==============================] - 2s 194us/step - loss: 7.1474e-05 - val_loss: 7.0999e-05\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00006\n",
      "Epoch 18/50\n",
      "8567/8567 [==============================] - 2s 191us/step - loss: 6.6875e-05 - val_loss: 6.6925e-05\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00006\n",
      "Epoch 19/50\n",
      "8567/8567 [==============================] - 2s 191us/step - loss: 6.8385e-05 - val_loss: 1.1407e-04\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00006\n",
      "Epoch 20/50\n",
      "8567/8567 [==============================] - 2s 196us/step - loss: 6.9045e-05 - val_loss: 5.8935e-05\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00006\n",
      "Epoch 21/50\n",
      "8567/8567 [==============================] - 2s 224us/step - loss: 6.7685e-05 - val_loss: 1.5563e-04\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x20131698780>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=50, callbacks=my_callbacks, validation_data=[X_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = scaler.inverse_transform(y_test)\n",
    "y_pred = scaler.inverse_transform(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.423828125"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_actual, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540 30 30\n",
      "(8540, 37) (30, 37) (30, 37)\n"
     ]
    }
   ],
   "source": [
    "valid_size = 30\n",
    "train_size = 30\n",
    "\n",
    "# split into train and test sets\n",
    "train, valid, test = scaled[0:(len(scaled)-valid_size-train_size),:], scaled[(len(scaled)-valid_size-train_size):(len(scaled)-valid_size),:], scaled[(len(scaled)-valid_size):,:]    \n",
    "print(len(train), len(valid), len(test))\n",
    "print(train.shape, valid.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.09090909, 0.09615385, 0.23333333, 0.83333333,\n",
       "       0.10410959, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.0433068 , 0.04011987, 0.00755519, 0.00729735, 0.03733373,\n",
       "       0.03419948, 0.        , 0.01430729, 0.03663004, 0.02022472,\n",
       "       0.03846154, 0.04032258, 0.03663004, 0.28362573, 0.5       ,\n",
       "       0.8111995 , 0.72334858, 0.03663004, 0.28362573, 0.03663004,\n",
       "       0.28362573])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03663003663003663"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8315018315018315"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8315018315018315\n",
      "0.8315018315018315\n",
      "0.8283516483516483\n",
      "0.8424175824175825\n",
      "0.8424175824175825\n",
      "0.8388278388278388\n",
      "0.8424908424908425\n",
      "0.8424908424908425\n",
      "0.8424908424908425\n",
      "0.8351648351648352\n",
      "0.8388278388278388\n",
      "0.8351648351648352\n",
      "0.8351648351648352\n",
      "0.8351648351648352\n",
      "0.8351648351648352\n",
      "0.8351648351648352\n",
      "0.8355311355311356\n",
      "0.8355311355311356\n",
      "0.8355311355311356\n",
      "0.8419047619047619\n",
      "0.8416849816849817\n",
      "0.8416849816849817\n",
      "0.8416849816849817\n",
      "0.8416117216117216\n",
      "0.8461538461538461\n",
      "0.8572161172161172\n",
      "0.8644688644688645\n",
      "0.8791208791208791\n",
      "0.8791208791208791\n",
      "0.8791208791208791\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(valid)):\n",
    "    print(valid[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write or reuse a function to segment the dataset in the appropriate format \n",
    "# Tip: function_name (dataset, n_steps_in)\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(df_arrays):\n",
    "    data_x, data_y = [], []\n",
    "    for i in range(len(df_arrays)):  \n",
    "        a = df_arrays[i, 1:len(df_7_days)]\n",
    "        data_x.append(a)\n",
    "        data_y.append(df_arrays[i, 0])\n",
    "    return np.array(data_x), np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.09090909 0.09615385 0.23333333 0.83333333 0.10410959\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0433068  0.04011987 0.00755519\n",
      " 0.00729735 0.03733373 0.03419948 0.         0.01430729 0.03663004\n",
      " 0.02022472 0.03846154 0.04032258 0.03663004 0.28362573 0.5\n",
      " 0.8111995  0.72334858 0.03663004 0.28362573 0.03663004 0.28362573] 0.03663003663003663\n",
      "[0.00000000e+00 9.09090909e-02 9.61538462e-02 2.66666667e-01\n",
      " 1.00000000e+00 1.06849315e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.16292592e-04 4.33067964e-02\n",
      " 4.01198670e-02 7.55519425e-03 7.29735302e-03 3.73337316e-02\n",
      " 3.40192115e-02 0.00000000e+00 1.35872117e-02 3.66300366e-02\n",
      " 2.02247191e-02 3.84615385e-02 4.03225806e-02 3.66300366e-02\n",
      " 2.83625731e-01 5.00000000e-01 8.11199502e-01 7.23348584e-01\n",
      " 3.66300366e-02 2.83625731e-01 3.66300366e-02 2.83625731e-01] 0.03663003663003663\n"
     ]
    }
   ],
   "source": [
    "### Use the function on the dataset and print the newly segmented data\n",
    "# use n_steps_in = 5\n",
    "train_X, train_y = create_dataset(train)\n",
    "valid_X, valid_y = create_dataset(valid)\n",
    "test_X, test_y = create_dataset(test)\n",
    "for i in range(2):\n",
    "    print(train_X[i], train_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8540, 36), (30, 36), (30, 36))"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, valid_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data to be appropriate for trianing the RNN model\n",
    "train_X = train_X.reshape(train_X.shape[0], 1, train_X.shape[1])\n",
    "valid_X = valid_X.reshape(valid_X.shape[0], 1, valid_X.shape[1])\n",
    "test_X = test_X.reshape (test_X.shape[0], 1, test_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8540, 1, 36), (30, 1, 36), (30, 1, 36))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, valid_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the RNN network. \n",
    "# Use LSTM with 60 neurons, RELU activation function, MAE as a loss, and SGD as optimizer.\n",
    "model = Sequential()\n",
    "model.add(LSTM(60, activation='relu', input_shape=(1, 36)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback1 = ModelCheckpoint('best_RNN.pt', monitor='loss', save_best_only=True, mode='min', verbose=1)\n",
    "callback2 = EarlyStopping(patience=15, monitor='loss', mode='min')\n",
    "callback_list = [callback1, callback2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: loss improved from inf to 0.11514, saving model to best_RNN.pt\n",
      "\n",
      "Epoch 00002: loss improved from 0.11514 to 0.03469, saving model to best_RNN.pt\n",
      "\n",
      "Epoch 00003: loss improved from 0.03469 to 0.01198, saving model to best_RNN.pt\n",
      "\n",
      "Epoch 00004: loss improved from 0.01198 to 0.01019, saving model to best_RNN.pt\n",
      "\n",
      "Epoch 00005: loss improved from 0.01019 to 0.00947, saving model to best_RNN.pt\n",
      "\n",
      "Epoch 00006: loss improved from 0.00947 to 0.00906, saving model to best_RNN.pt\n",
      "\n",
      "Epoch 00007: loss improved from 0.00906 to 0.00878, saving model to best_RNN.pt\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.00878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2298846dcf8>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, epochs=50,callbacks=callback_list, verbose=0, validation_data=(valid_X, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1, 36)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8867388 ],\n",
       "       [0.896636  ],\n",
       "       [0.88847417],\n",
       "       [0.9030396 ],\n",
       "       [0.90821034]], dtype=float32)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions\n",
    "test_predict = model.predict(test_X)\n",
    "test_predict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 1), (30,))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (30,1) doesn't match the broadcast shape (30,37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-aef3daacd9ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# invert predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_predict_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    434\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (30,1) doesn't match the broadcast shape (30,37)"
     ]
    }
   ],
   "source": [
    "# invert predictions\n",
    "test_predict_ = scaler.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
